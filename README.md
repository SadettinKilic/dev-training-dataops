<p align="left">
  <strong>Language:</strong>
    <img src="https://img.shields.io/badge/T%C3%BCrk%C3%A7e-red">
  <a href="./README_EN.md">
    <img src="https://img.shields.io/badge/English-lightgrey">
  </a>
</p>
<p align="left">
  <!-- Core Tech -->
  <img src="https://img.shields.io/badge/dbt-Analytics-orange">
  <img src="https://img.shields.io/badge/Microsoft%20Azure-Cloud-blue">

  <!-- Architecture -->
  <img src="https://img.shields.io/badge/Architecture-Medallion-success">

  <!-- REAL CI BADGE -->
  <img src="https://github.com/SadettinKilic/dev-training-dataops/actions/workflows/dbt_ci.yml/badge.svg">

  <!-- DataOps -->
  <img src="https://img.shields.io/badge/DataOps-Automated-informational">
</p>

* * *

# ğŸ… Paris 2024 Olympics DataOps & Analytics Project

Bu proje, Paris 2024 Yaz Olimpiyat OyunlarÄ± verilerini kullanarak **End-to-End (UÃ§tan Uca)** bir Data Engineering pipeline'Ä± oluÅŸturmayÄ± hedefler. Azure ekosistemi Ã¼zerinde **Medallion Architecture** prensiplerine uygun olarak tasarlanmÄ±ÅŸ; Docker, ADF, Databricks ve dbt teknolojilerini bir araya getiren modern, konteynerize edilmiÅŸ bir veri platformudur.
* * *
## ğŸ—ï¸ Mimari ve Kapsam
Proje, verinin ham halinden (CSV/Parquet) raporlanabilir AltÄ±n (Gold) tablolar haline gelene kadar geÃ§tiÄŸi tÃ¼m sÃ¼reÃ§leri kapsar.
### **Teknoloji Stack'i**
-   **Konteynerizasyon:** Docker (Portable dbt Runtime)
-   **Orkestrasyon:** Azure Data Factory (ADF)
-   **Veri AmbarÄ± & Ä°ÅŸleme:** Azure Databricks (Spark & Serverless SQL Warehouse)
-   **Transformasyon:** dbt (data build tool)
-   **Veri GÃ¶lÃ¼:** Azure Data Lake Storage Gen2 (ADLS Gen2)
-   **GÃ¼venlik:** Azure Key Vault & Managed Identity (RBAC)
-   **SÃ¼rÃ¼m KontrolÃ¼:** GitHub (ADF & dbt integration)
### ğŸ“‚ Proje DepolarÄ± (Repositories)

Proje, DataOps prensipleri gereÄŸi transformasyon kodlarÄ± ve orkestrasyon yapÄ±landÄ±rmasÄ± olarak iki ayrÄ± depoda yÃ¶netilmektedir:

| **Repository** | **Ä°Ã§erik** | **Link** |
| --- | --- | --- |
| **dbt Repository** | dbt Modelleri, SQL logic, CI/CD (SQLFluff, Freshness) | [ğŸ”— dev-training-dataops](https://github.com/SadettinKilic/dev-training-dataops) |
| **ADF Repository** | Azure Data Factory Pipeline'larÄ±, JSON tanÄ±mlarÄ±, Linked Services, Triggers | [ğŸ”— dev-training-dataops-adf](https://github.com/SadettinKilic/dev-training-dataops-adf) |
* * *
## ğŸ“‚ Azure Kaynak YapÄ±sÄ± (Resource Group: `rg-training-dataops`)

| **Kaynak AdÄ±** | **TÃ¼rÃ¼** | **GÃ¶revi** |
| --- | --- | --- |
| `sttrainingdataops` | Storage Account | Bronze, Silver, Gold katmanlarÄ±nÄ± barÄ±ndÄ±ran veri gÃ¶lÃ¼. |
| `kv-training-dataops` | Key Vault | Token ve baÄŸlantÄ± bilgilerinin gÃ¼venli depolanmasÄ±. |
| `dev-training-dataops-adf` | Data Factory | Pipeline'larÄ±n yÃ¶netimi ve zamanlanmasÄ±. |
| `dbx-training-dataops-dev` | Databricks | dbt modellerinin Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ± ana iÅŸlem motoru. |
| `dbx-connector-training-dataops-dev` | Access Connector | Databricks'in Storage'a eriÅŸimi iÃ§in yÃ¶netilen kimlik. |

* * *

## ğŸš€ Kurulum ve Uygulama AdÄ±mlarÄ±

Bu projeyi sÄ±fÄ±rdan ayaÄŸa kaldÄ±rmak iÃ§in aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyin:

### 1\. AltyapÄ± ve Veri KatmanlarÄ±nÄ±n HazÄ±rlanmasÄ±

Storage Account Ã¼zerinde aÅŸaÄŸÄ±daki container yapÄ±sÄ±nÄ± oluÅŸturun:
-   `source/raw_data`: Kaggle'dan indirilen ham CSV dosyalarÄ±.
-   `bronze`, `silver`, `gold`: Ä°ÅŸlenmiÅŸ veri katmanlarÄ±.
-   `dbx-managed`: Databricks Managed Catalog iÃ§in ayrÄ±lmÄ±ÅŸ alan.
-   `monitoring`: monitoring tablolarÄ±nÄ± iÃ§eriyor. Dashboard iÃ§in kullanÄ±lacak.

### 2\. IAM ve GÃ¼venlik YapÄ±landÄ±rmasÄ± (Ã–nemli)

Azure Ã¼zerinde servislerin birbiriyle konuÅŸabilmesi iÃ§in ÅŸu yetkileri tanÄ±mlayÄ±n:
-   **Storage Account:** Databricks Access Connector'a `Storage Blob Data Contributor` yetkisi verin.
-   **Key Vault:** ADF'e ÅŸifreleri okuyabilmesi iÃ§in `Key Vault Secrets User` yetkisi verin. Kendi kullanÄ±cÄ±nÄ±za ise `Key Vault Administrator` yetkisi tanÄ±mlayÄ±n.
-   **RBAC:** TÃ¼m kaynaklarÄ± tek bir Resource Group altÄ±nda toplayarak eriÅŸim yÃ¶netimini merkezileÅŸtirin.
    

### 3\. Databricks Katalog ve Åema Kurulumu

Databricks SQL Editor Ã¼zerinden Unity Catalog yapÄ±sÄ±nÄ± kurun (Ã–ncesinde external locationlarÄ± oluÅŸturmanÄ±z gerekecektir):
```sql
    CREATE CATALOG IF NOT EXISTS dataops MANAGED LOCATION 'abfss://dbx-managed@sttrainingdataops.dfs.core.windows.net/';
    USE CATALOG dataops; 
    
    CREATE SCHEMA IF NOT EXISTS bronze MANAGED LOCATION 'abfss://bronze@sttrainingdataops.dfs.core.windows.net/';
    CREATE SCHEMA IF NOT EXISTS silver MANAGED LOCATION 'abfss://silver@sttrainingdataops.dfs.core.windows.net/';
    CREATE SCHEMA IF NOT EXISTS gold MANAGED LOCATION 'abfss://gold@sttrainingdataops.dfs.core.windows.net/';
    CREATE SCHEMA IF NOT EXISTS monitoring MANAGED LOCATION 'abfss://monitoring@sttrainingdataops.dfs.core.windows.net/';
```
### 4. Veri TanÄ±mlama ve Bronze Tablo YapÄ±larÄ±

###   
Bronze katmanÄ±ndaki tablolar, ham verilerin (CSV/Parquet) ÅŸemalarÄ±nÄ± koruyarak Unity Catalog altÄ±nda ÅŸu ÅŸekilde tanÄ±mlanmÄ±ÅŸtÄ±r:

### 
```sql
    /* Katalog ve Åema BaÄŸlamÄ±nÄ± Ayarla */
    USE CATALOG dataops;
    USE SCHEMA bronze;
    
    /* Sporcu Tablosu (Parquet FormatÄ±) */
    CREATE TABLE IF NOT EXISTS bronze.raw_athletes
    USING PARQUET
    LOCATION 'abfss://bronze@sttrainingdataops.dfs.core.windows.net/athletes/';
    
    /* AntrenÃ¶r Tablosu (Parquet FormatÄ±) */
    CREATE TABLE IF NOT EXISTS bronze.raw_coaches
    USING PARQUET
    LOCATION 'abfss://bronze@sttrainingdataops.dfs.core.windows.net/coaches/';
    
    /* Etkinlik Tablosu (Parquet FormatÄ±) */
    CREATE TABLE IF NOT EXISTS bronze.raw_events
    USING PARQUET
    LOCATION 'abfss://bronze@sttrainingdataops.dfs.core.windows.net/events/';
    
    /* NOC (Milli Olimpiyat Komiteleri) Tablosu (CSV FormatÄ±) */
    CREATE TABLE IF NOT EXISTS bronze.raw_nocs
    USING CSV
    OPTIONS (header='true', inferSchema='true')
    LOCATION 'abfss://bronze@sttrainingdataops.dfs.core.windows.net/nocs/';

    CREATE TABLE IF NOT EXISTS dataops.monitoring.audit_logs (
      model_name STRING,
      execution_time TIMESTAMP,
      row_count LONG,
      status STRING
    ) USING DELTA;
```
### 5\. ADF Pipeline YapÄ±landÄ±rmasÄ±
ADF Ã¼zerinde iki ana sÃ¼reÃ§ yÃ¶netilmektedir:
-   **Ingestion:** `raw_to_bronze` pipeline'Ä±, `bronze/param.json` dosyasÄ±ndaki metadata'yÄ± okuyarak ham verileri dinamik olarak Bronze katmanÄ±na taÅŸÄ±r.
-   **Transformation:** `dbt_dataops_gold_daily` pipeline'Ä±, Key Vault Ã¼zerinden aldÄ±ÄŸÄ± token ile Databricks API'sini tetikler ve dbt modellerini koÅŸturur.  

### 6\. dbt (Data Build Tool) Entegrasyonu
Bu repoyu bilgisayarÄ±nÄ±za Ã§ekin.
Proje dizininde yer alan `.env.example`  dosyasÄ±nÄ± Databricks baÄŸlantÄ± bilgilerinizle gÃ¼ncelleyin ve aÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rÄ±n:
```bash
    mv .env.example .env
```
Docker Compose kullanarak tÃ¼m dbt modellerini Databricks Ã¼zerinde koÅŸturabilirsiniz:
```bash
    docker-compose up
```
### 3\. CI/CD SÃ¼reci (GitHub Actions)
### 
Proje, her kod deÄŸiÅŸikliÄŸinde otomatik olarak devreye giren geliÅŸmiÅŸ bir pipeline yapÄ±sÄ±na sahiptir:
-   **Static Analysis:** `sqlfluff` ile SQL kod standartlarÄ± denetlenir ve otomatik dÃ¼zeltilir.
-   **Containerized Execution:** dbt komutlarÄ±, GitHub Actions Ã¼zerinde Ã¶zel olarak hazÄ±rlanmÄ±ÅŸ `dataops-dbt-runner` Docker imajÄ± iÃ§inde koÅŸturulur. Bu sayede kurulum sÃ¼releri %70 oranÄ±nda optimize edilmiÅŸtir.
-   **Automated Docs:** Her baÅŸarÄ±lÄ± run sonrasÄ± dbt dokÃ¼mantasyonu otomatik olarak Ã¼retilir ve **GitHub Pages** Ã¼zerinde yayÄ±nlanÄ±r.
* * *

### ğŸ“Š Ä°zleme ve GÃ¶zlemlenebilirlik (DataOps Dashboard)
### 
Projenin saÄŸlÄ±ÄŸÄ±, performansÄ± ve veri kalitesi **Databricks SQL Dashboard** Ã¼zerinden anlÄ±k olarak takip edilmektedir.
-   **Pipeline GÃ¼venilirliÄŸi:** GÃ¼nlÃ¼k baÅŸarÄ±lÄ±/hatalÄ± model Ã§alÄ±ÅŸmalarÄ±.  
-   **Model PerformansÄ± (Wall of Shame):** En Ã§ok kaynak tÃ¼keten ve optimizasyon gerektiren modellerin tespiti.
-   **Veri AkÄ±ÅŸ HÄ±zÄ± (Throughput):** Saniyede iÅŸlenen satÄ±r sayÄ±sÄ± bazÄ±nda SQL verimlilik analizi.
-   **Veri Hacmi Drift Analizi:** Kaynak sistemlerden gelen veri miktarÄ±ndaki ani deÄŸiÅŸimlerin takibi.

### ğŸ“– CanlÄ± DÃ¶kÃ¼mantasyon ve Veri SoyaÄŸacÄ± (Lineage)
### 
Projenin teknik detaylarÄ± ve tablolar arasÄ± iliÅŸkiler **dbt Docs** ile otomatik olarak belgelenmektedir.
-   **[dbt Docs SayfasÄ±](https://sadettinkilic.github.io/dev-training-dataops/)** SaÄŸ alttaki 'Lineage' butonuna tÄ±klayarak akÄ±ÅŸ ÅŸemasÄ±nÄ± inceleyebilirsiniz.
-   **Ä°nteraktif SoyaÄŸacÄ±:** Bronze -> Silver -> Gold katmanlarÄ± arasÄ±ndaki veri akÄ±ÅŸÄ±nÄ± gÃ¶rsel olarak inceleyebilirsiniz.
-   **Veri KataloÄŸu:** Tablo ÅŸemalarÄ±, sÃ¼tun aÃ§Ä±klamalarÄ± ve uygulanan dbt testleri.
  
* * *

## âš™ï¸ Ekstra KonfigÃ¼rasyon NotlarÄ±

Projeyi kendi ortamÄ±nda Ã§alÄ±ÅŸtÄ±rmak isteyenlerin ÅŸu ayarlarÄ± yapmasÄ± gerekir:

-   **GitHub Secrets:** GitHub deponuzun `Settings > Secrets and variables > Actions` kÄ±smÄ±na aÅŸaÄŸÄ±daki deÄŸiÅŸkenleri eklemelisiniz:
    -   `DATABRICKS_HOST`: Databricks instance URL'iniz.
    -   `DATABRICKS_HTTP_PATH`: SQL Warehouse HTTP yolu.
    -   `DATABRICKS_TOKEN`: Key Vault'ta da sakladÄ±ÄŸÄ±nÄ±z eriÅŸim token'Ä±.
        
-   **Environment Variables:** dbt'nin bu secret'lara eriÅŸebilmesi iÃ§in `profiles.yml` dosyasÄ±nda env\_var kullanÄ±mÄ± (Ã–rn: `{{ env_var('DBT_HOST') }}`) yapÄ±landÄ±rÄ±lmÄ±ÅŸtÄ±r.
    

* * *

## ğŸ› ï¸ Projenin Ã–ne Ã‡Ä±kan Yetenekleri
-   **Portable Runtime (Docker):** GeliÅŸtirme ortamÄ± ile canlÄ± ortam arasÄ±ndaki farklarÄ± sÄ±fÄ±ra indiren konteyner yapÄ±sÄ±.
-   **Dinamik Ingestion:** Yeni bir tablo eklemek iÃ§in kod yazmaya gerek kalmadan sadece `param.json` dosyasÄ±nÄ± gÃ¼ncellemek yeterlidir.
-   **Secure Secrets:** HiÃ§bir ÅŸifre veya token kodun iÃ§erisinde (hardcoded) bulunmaz; tamamen Azure Key Vault Ã¼zerinden dinamik olarak Ã§ekilir.
-   **Medallion Architecture:** Veri kalitesi her katmanda artÄ±rÄ±larak (Raw -> Bronze -> Silver -> Gold) gÃ¼venilir bir "Single Source of Truth" oluÅŸturulur.
-   **Zero-Install CI:** GitHub Actions'Ä±n boÅŸ makinelerde kÃ¼tÃ¼phane kurmak yerine hazÄ±r Docker imajlarÄ±nÄ± kullanarak hÄ±zlanmasÄ±.
-   **Automated DataOps:** GitHub Actions ile kod kalitesi (SQLFluff) ve veri tazeliÄŸi (dbt Freshness) otomatik olarak denetlenerek hata payÄ± minimize edilmiÅŸtir.
-   **Separation of Concerns:** Veri dÃ¶nÃ¼ÅŸÃ¼m mantÄ±ÄŸÄ± (dbt) ile veri taÅŸÄ±ma/orkestrasyon mantÄ±ÄŸÄ± (ADF) ayrÄ± depolarda yÃ¶netilerek modÃ¼ler ve bakÄ±mÄ± kolay bir yapÄ± sunulur.

* * *

## ğŸ“ˆ Veri Seti HakkÄ±nda

Projede kullanÄ±lan veri seti Kaggle'daki [Paris 2024 Olympic Summer Games](https://www.kaggle.com/datasets/piterfm/paris-2024-olympic-summer-games) setidir. Sporcular, antrenÃ¶rler, madalyalar ve etkinlikler hakkÄ±nda detaylÄ± bilgiler iÃ§erir.

* * *

_Bu dÃ¶kÃ¼man, modern DataOps prensiplerine sadÄ±k kalÄ±narak hazÄ±rlanmÄ±ÅŸtÄ±r._

